{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccbd6b8",
   "metadata": {},
   "source": [
    "# Creating a Chatbot with Milvus\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Virtual environment and Python libraries\n",
    "\n",
    "If you're reading this, you may have already followed the instruction to create a virtual environment and loaded the libraries required for this project.\n",
    "\n",
    "If not, here they are:\n",
    "\n",
    "Create a virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f316af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m venv ./chatbot_venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bb4fa-f13e-4d54-9d93-0c3a73f968d4",
   "metadata": {},
   "source": [
    "Source your new environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373bfdfb-7ee1-4d81-95f0-6cbdd3e85cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source chatbot_venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5acc75d-0ada-4954-bc8c-8a61dcf009d4",
   "metadata": {},
   "source": [
    "Upgrade pip to the latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f272ea-5434-40fb-bf69-48bddfcc5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62716480-5e15-47be-85e9-e0b87b919cfc",
   "metadata": {},
   "source": [
    "Install these libraries: pandas, langchain, towhee, unstructured, milvus, pymilvus, sentence_transformers, gradio, openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51550df1-5177-4706-8785-d376062b330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas langchain towhee unstructured milvus pymilvus sentence_transformers gradio openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5937ca3",
   "metadata": {},
   "source": [
    "### Document store\n",
    "\n",
    "You'll use Milvus to store both document chunks and their corresponding embeddings. This makes it easy to perform semantic retrieval via vector similarity.\n",
    "\n",
    "**Note**: You can skip this section if you already have a Milvus server ready locally, or via Zilliz.\n",
    "\n",
    "If you don't have a server, let's use [Milvus Lite](https://milvus.io/docs/milvus_lite.md), a lightweight version of Milvus that works seamlessly with Jupyter Notebook (or any other app that uses a client/server connection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from milvus import default_server\n",
    "\n",
    "# Start Milvus service\n",
    "default_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36986288-62d6-4f17-be17-ba79adee57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use this block to stop the server if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff27c3-ab01-4949-bb90-71368b661d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Milvus service\n",
    "default_server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473b05f-c3fc-440c-9c3e-abcee234aaf5",
   "metadata": {},
   "source": [
    "### Set Variables\n",
    "\n",
    "Next, let's set a few variables.\n",
    "\n",
    "- The URIs for Milvus and SQLite (which will act as a memory store for chat.)\n",
    "- A flag to drop existing Milvus collections\n",
    "- The embed model for processing text. Change this variable to experiement with different models\n",
    "- Your Milvus collection name. \n",
    "- The size of your embedding arrays\n",
    "- Your OpenAI key\n",
    "- Path to SQLite storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "MILVUS_URI = 'http://localhost:19530'\n",
    "[MILVUS_HOST, MILVUS_PORT] = MILVUS_URI.split('://')[1].split(':')\n",
    "DROP_EXIST = True\n",
    "EMBED_MODEL = 'all-mpnet-base-v2'\n",
    "COLLECTION_NAME = 'chatbot_demo'\n",
    "DIM = 768\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "# Clean up chat history from any previous runs\n",
    "if os.path.exists('./sqlite.db'):\n",
    "    os.remove('./sqlite.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e9e38",
   "metadata": {},
   "source": [
    "Run this and respond to the prompt for your OpenAI key.\n",
    "\n",
    "## Build Knowledge Base\n",
    "\n",
    "Now you need a pipeline that can load documents and split them into smaller checks. Then take the chunks and use a pretrained model to extract text embedding for each doc chunk.\n",
    "\n",
    "**Load & split documents**\n",
    "\n",
    "This sample code block processes the Twowhee home page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d817fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipe, ops, DataCollection\n",
    "\n",
    "pipe_load = (\n",
    "    pipe.input('source')\n",
    "        .map('source', 'doc', ops.text_loader())\n",
    "        .flat_map('doc', 'doc_chunks', ops.text_splitter(chunk_size=300))\n",
    "        .output('source', 'doc_chunks')\n",
    ")\n",
    "DataCollection(pipe_load('https://towhee.io')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752be964",
   "metadata": {},
   "source": [
    "**Text to Embedding**\n",
    "\n",
    "Using a pretrained model, you convert each text chunk into an embedding. These embeddings enable semantic search. As highluighted abovie, switch models by adjusting EMBED_MODEL to the name of another pretrained model supported by Sentence Transformers.\n",
    "\n",
    "For example, convert the following doc piece to a text embedding:\n",
    "\n",
    "> SOTA Models\n",
    ">\n",
    "> We provide 700+ pre-trained embedding models spanning 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures. These include BERT, CLIP, ViT, SwinTransformer, data2vec, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f71ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_embed = (\n",
    "    pipe.input('doc_chunk')\n",
    "        .map('doc_chunk', 'vec', ops.sentence_embedding.sbert(model_name=EMBED_MODEL))\n",
    "        .map('vec', 'vec', ops.np_normalize())\n",
    "        .output('doc_chunk', 'vec')\n",
    ")\n",
    "\n",
    "text = '''SOTA Models\n",
    "\n",
    "We provide 700+ pre-trained embedding models spanning 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures.\n",
    "These include BERT, CLIP, ViT, SwinTransformer, data2vec, etc.\n",
    "'''\n",
    "\n",
    "DataCollection(pipe_embed(text)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887ed75",
   "metadata": {},
   "source": [
    "**Insert data**\n",
    "\n",
    "Now it's time to insert the text and embeddings into Milvus. First, you need a collection. This code creates one, and will destroy any existing collections if DROP_EXIST is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    connections, utility, Collection,\n",
    "    CollectionSchema, FieldSchema, DataType\n",
    ")\n",
    "\n",
    "\n",
    "def create_collection(collection_name):\n",
    "    connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    \n",
    "    has_collection = utility.has_collection(collection_name)\n",
    "    \n",
    "    if has_collection:\n",
    "        collection = Collection(collection_name)\n",
    "        if DROP_EXIST:\n",
    "            collection.drop()\n",
    "        else:\n",
    "            return collection\n",
    "\n",
    "    # Create collection\n",
    "    fields = [\n",
    "        FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIM),\n",
    "        FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=500)\n",
    "    ]\n",
    "    schema = CollectionSchema(\n",
    "        fields=fields,\n",
    "        description=\"Towhee demo\",\n",
    "        enable_dynamic_field=True\n",
    "        )\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    \n",
    "    # Change index here if you want to accelerate search\n",
    "    \n",
    "    index_params = {\n",
    "        'metric_type': 'IP',\n",
    "        'index_type': 'IVF_FLAT',\n",
    "        'params': {'nlist': 1024}\n",
    "        }\n",
    "    collection.create_index(\n",
    "        field_name='embedding', \n",
    "        index_params=index_params\n",
    "    )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24f75a",
   "metadata": {},
   "source": [
    "Next, you need a pipeline to create the collection and insert vectors and doc chunks into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipe, ops, DataCollection\n",
    "\n",
    "pipe_insert = (\n",
    "    pipe.input('collection_name', 'vec', 'doc_chunk')\n",
    "        .map(('collection_name', 'vec', 'doc_chunk'), 'mr',\n",
    "             ops.ann_insert.osschat_milvus(host=MILVUS_HOST, port=MILVUS_PORT))\n",
    "        .output('mr')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae446499",
   "metadata": {},
   "source": [
    "### Complete Insert Pipeline\n",
    "\n",
    "When you assemble the operators above, you have an end-to-end pipeline that builds your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10c0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from towhee import pipe, ops, DataCollection\n",
    "\n",
    "load_data = (\n",
    "    pipe.input('collection_name', 'source')\n",
    "        .map('collection_name', 'collection', create_collection)\n",
    "        .map('source', 'doc', ops.text_loader())\n",
    "        .flat_map('doc', 'doc_chunk', ops.text_splitter(chunk_size=300))\n",
    "        .map('doc_chunk', 'vec', ops.sentence_embedding.sbert(model_name=EMBED_MODEL))\n",
    "        .map('vec', 'vec', ops.np_normalize())\n",
    "        .map(('collection_name', 'vec', 'doc_chunk'), 'mr',\n",
    "             ops.ann_insert.osschat_milvus(host=MILVUS_HOST, port=MILVUS_PORT))\n",
    "        .output('mr')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1af2a",
   "metadata": {},
   "source": [
    "Use the pipeline to build a knowledge base with the Wikipedia page for [Frodo Baggins](https://en.wikipedia.org/wiki/Frodo_Baggins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02451b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_source = 'https://en.wikipedia.org/wiki/Frodo_Baggins'\n",
    "mr = load_data(COLLECTION_NAME, data_source)\n",
    "\n",
    "print('Doc chunks inserted:', len(mr.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4cb73",
   "metadata": {},
   "source": [
    "## Search Knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_search = (\n",
    "    pipe.input('collection_name', 'query')\n",
    "        .map('query', 'query_vec', ops.sentence_embedding.sbert(model_name=EMBED_MODEL))\n",
    "        .map('query_vec', 'query_vec', ops.np_normalize())\n",
    "        .map(('collection_name', 'query_vec'), 'search_res',\n",
    "             ops.ann_search.osschat_milvus(host=MILVUS_HOST,\n",
    "                                           port=MILVUS_PORT,\n",
    "                                           **{'metric_type': 'IP', 'limit': 3, 'output_fields': ['text']}))\n",
    "        .flat_map('search_res', ('id', 'score', 'text'), lambda x: (x[0], x[1], x[2]))\n",
    "        .output('query', 'text', 'score')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7c0c3",
   "metadata": {},
   "source": [
    "For example, search knowledge for the question \"What is Towhee?\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0591ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who is Frodo Baggins?'\n",
    "DataCollection(pipe_search(COLLECTION_NAME, query)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97592dcc-b701-4e1c-ac22-5d34af4d7303",
   "metadata": {},
   "source": [
    "## Chat History\n",
    "\n",
    "You'll need to input chat history to the LLM to get effective responses. For this, SQLite is a good choice.\n",
    "\n",
    "First, here's a function to retrieve chat history. It reuses the Milvus collection name for the SQLite table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0fec6-7e58-4ded-8495-0e9fe6228b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_get_history = (\n",
    "    pipe.input('collection_name', 'session')\n",
    "        .map(('collection_name', 'session'), 'history', ops.chat_message_histories.sql(method='get'))\n",
    "        .output('collection_name', 'session', 'history')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9563a1d-b352-49cb-b06e-f16b6440ab0a",
   "metadata": {},
   "source": [
    "And here's one to store it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f612a-ca35-4776-83a5-ec37750b5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_add_history = (\n",
    "    pipe.input('collection_name', 'session', 'question', 'answer')\n",
    "        .map(('collection_name', 'session', 'question', 'answer'), 'history', ops.chat_message_histories.sql(method='add'))\n",
    "        .output('history')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4160b0f-3628-43ff-97f1-02018fbb8c41",
   "metadata": {},
   "source": [
    "Let's test the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e350a-8f01-4e5b-81c9-2d5e3673fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = 'Test answer'\n",
    "session_id = 'sess01'\n",
    "\n",
    "pipe_add_history(COLLECTION_NAME, session_id, query, answer)\n",
    "\n",
    "# Check history\n",
    "pipe_get_history(COLLECTION_NAME, session_id).get_dict()['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23724ef-6efd-4aca-af80-ef7c5b282709",
   "metadata": {},
   "source": [
    "## Query Pipeline for Milvus and the LLM\n",
    "\n",
    "Now, let's use Milvus with the LLM to create the Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca8577-5695-4dba-8095-ac4f87aadb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = (\n",
    "    pipe.input('collection_name', 'query', 'session')\n",
    "        .map('query', 'query_vec', ops.sentence_embedding.sbert(model_name=EMBED_MODEL))\n",
    "        .map('query_vec', 'query_vec', ops.np_normalize())\n",
    "        .map(('collection_name', 'query_vec'), 'search_res',\n",
    "             ops.ann_search.osschat_milvus(host=MILVUS_HOST,\n",
    "                                           port=MILVUS_PORT,\n",
    "                                           **{'metric_type': 'IP', 'limit': 3, 'output_fields': ['text']}))\n",
    "        .map('search_res', 'knowledge', lambda y: [x[2] for x in y])\n",
    "        .map(('collection_name', 'session'), 'history', ops.chat_message_histories.sql(method='get'))\n",
    "        .map(('query', 'knowledge', 'history'), 'messages', ops.prompt.question_answer())\n",
    "        .map('messages', 'answer', ops.LLM.OpenAI(api_key=OPENAI_API_KEY,\n",
    "                                                  model_name='gpt-3.5-turbo',\n",
    "                                                  temperature=0.8))\n",
    "        .map(('collection_name', 'session', 'query', 'answer'), 'new_history', ops.chat_message_histories.sql(method='add'))\n",
    "        .output('query', 'history', 'answer', )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932e3f6-2bb7-4a19-a517-c5a01fdbe668",
   "metadata": {},
   "source": [
    "Give it a try with a new query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a955466-5cbc-4588-a8e8-e90f2a8616c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = 'Where did Frodo take the ring?'\n",
    "DataCollection(chat(COLLECTION_NAME, new_query, session_id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6392495-fb99-4b0c-8aea-0ea5d67226bc",
   "metadata": {},
   "source": [
    "## Gradio Chat Service\n",
    "\n",
    "Finally, let's run the bot with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27c17a-1a3f-458d-950f-bc7d2dfbfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import io\n",
    "\n",
    "def create_session_id():\n",
    "    uid = str(uuid.uuid4())\n",
    "    suid = ''.join(uid.split('-'))\n",
    "    return 'sess_' + suid\n",
    "\n",
    "\n",
    "def respond(session, query):\n",
    "    res = chat(COLLECTION_NAME, query, session).get_dict()\n",
    "    answer = res['answer']\n",
    "    response = res['history']\n",
    "    response.append((query, answer))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2515dc2-d676-470f-ae20-bb6c9cff0184",
   "metadata": {},
   "source": [
    "And the Gradio code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60015daa-abd0-4365-992e-77d1fa6dc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    session_id = gr.State(create_session_id)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown('''## Chat''')\n",
    "            conversation = gr.Chatbot(label='conversation').style(height=300)\n",
    "            question = gr.Textbox(label='question', value=None)\n",
    "    \n",
    "            send_btn = gr.Button('Send Message')\n",
    "            send_btn.click(\n",
    "                fn=respond,\n",
    "                inputs=[\n",
    "                    session_id,\n",
    "                    question\n",
    "                ],\n",
    "                outputs=conversation,\n",
    "            )\n",
    "\n",
    "demo.launch(server_name='127.0.0.1', server_port=8902)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a6f2d-3d87-4887-8f46-0dca27a04b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05425e-5c6e-4397-a9ad-bd73e1bca08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}